# 概览

<!-- 第一周 -->

## 课程简介

- 本课程主要关注极大规模的挖掘（data min-ing），极大规模是这些数据无法在内存中存放。
- 本课程主重点强调数据规模（极大），很多例子都是来自于 web 本身. （数据量很大）
- 本课程从算法角度来看待数据挖掘，即数据挖掘是将算法应用于数据，而不是使用数据来“训练”某种机器学习引擎.
- 本课程侧重于 web+数据挖掘+分布式处理

### 课程主要内容

- Web 数据挖掘的基本概念
- Map-Reduce 及其协议栈
- 相似项发现（哈希关键技术）
- 数据流挖掘
- 搜索引擎技术
- 频繁项集挖掘
- 大规模高维数据集的聚类

## 主要内容

## 1 数据挖掘基本概念

**数据挖掘**是**数据”模型“（model）的发现**的过程。模型，能够实现的，生成的。用数学语言描述的一类模型。
数学模型可以是一个或一组代数方程、微分方程、差分方程、积分方程或统计学方程，也可以是它们的某种适当的组合

从数据中抽取知识需要：存储，管理和**分析**。数据挖掘 ≈ 大数据 ≈ 预测分析 ≈ 数据科学。

### 1.1 数据挖掘基本概念

#### 统计建模

统计学习理论是一种研究训练样本有限情况下的机器学习规律的学科。
**统计建模**是以*计算机统计分析软件*为工具，利用各种*统计分析方法*对批量数据建立统计模型和探索处理的过程

#### 机器学习 vs 数据挖掘

异中求同 同中求异（目标不同，算法相同）

![](../../pics/2022-01-04-21-17-49.png)

**机器学习**是一门多学科交叉专业，涵盖概率论知识，统计学知识，近似理论知识和复杂算法知识，使用**计算机作为工具**并致力于真实实时的**模拟人类学习**方式， 并**将现有内容进行知识结构划分**来有效提高学习效率。
**机器学习三要素**： 模型，策略，算法。

数据挖掘技术是机器学习算法和数据存取技术的结合，利用机器学习提供的统计分析、知识发现等手段分析海量数据，同时利用数据存取机制实现数据的高效读写。

#### 建模的计算方法 Computational Approaches to Modeling

##### 数据汇总 Summarization

1. PageRank, chap. 5：**网页排名**，又称网页级别、Google 左侧排名或佩奇排名，是一种由根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一
2. cluster, chap. 7：将物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为**聚类**。聚类分析又称群分析，它是研究（样品或指标）分类问题的一种统计分析方法。聚类分析起源于分类学，但是聚类不等于分类。聚类与分类的不同在于，聚类所要求划分的类是未知的。如：伦敦霍乱

##### 特征抽取 Feature Extraction

###### 频繁项集, chap. 6

- 项集：最基本的模式是项集，它是指若干个项的集合。
- 频繁模式是指数据集中频繁出现的项集、序列或子结构。
- 频繁项集是指支持度大于等于最小支持度(min_sup)的集合。其中支持度是指某个集合在所有事务中出现的频率。频繁项集的经典应用是购物篮模型。

###### 相似项 Similar item, chap. 3

**协同过滤** Collaborative filtering
例如：基于用户的协同过滤算法是通过用户的历史行为数据发现用户对商品或内容的喜欢(如商品购买，收藏，内容评论或分享)，并对这些喜好进行度量和打分。

### 1.2 数据挖掘的统计限制

#### 没有免费午餐定理

**没有免费午餐定理**(No Free Lunch，简称 NFL)是 wolpert 和 Macerday 提出的“最优化理论的发展”之一：在机器学习算法中的体现为**在没有实际背景下，没有一种算法比随机胡猜的效果好**

1. 欠拟合 High bias(uderfit)：拟合偏差比较大，（没有学到所有共有特征，树->树叶）
2. "just right"
3. 过拟合 High variance：拟合偏差小，但是结果不符合认知，在新的测试样本情况下，很大可能不会落在拟合曲线上，（将样本独有特征学了，树叶无锯齿->非树叶）

#### 邦弗朗尼原理

**数据挖掘所谓发现的模型可能是没有任何意义的**，统计上称为**邦弗朗尼原理** 。例如美国提出的整体情报预警 TIA 可能没有意义

<!-- 第二周 -->

### 相关知识

- **词语在文档中的重要性**：TF.IDF 词项频率乘以逆文档频率 Term Frequency times Inverse Document Frequency：用于度量词语重要性

- 哈希函数 Hash function: 输入是键值 hash-key，输出是桶编号 bucket number 通常为一个整数。通常会使用$h(x) = x\  mod \ B$通常使用质数作为 B，使得分配近似均匀分布

- 索引 index

- 二级存储器 secondary Storage: 通常指硬盘光盘。容量大，断电不易失去，读取速度慢（相对于 RAM 内存）

- e 和幂定理 The base of Natural Logarithms and Power Law

假设词 $i$ 出现在 $N$ 个文档中的 $n_i$个文档中. 定义 $f_{ij}$ 为在文档 $j$ 中词$i$出现的频率（次数），然后词项$i$在文档$j$中的 **词项频率乘以逆文档频率** 为：

$$
TF.IDF=TF_{ij}\times IDF_i=\frac{f_{ij}}{max_{k}f_{kj}}\times\log_2\frac{N}{n_i}


$$

<!-- ## 第三周-分布式文件系统 -->

自然对数的底数 e，e 是当 x 趋向于无穷大时，$(1+\frac{1}{x})^x$的极限

$$
e^x=\sum_{i=0}^{\infty}\frac{x^i}{i!}=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots
$$

## 2 MapReduce 及新协议栈

- 大数据如何存储：GFS、HDFS 等分布式文件系统

- 大数据如何计算：MapReduce 分布式计算框架

### 2.1 分布式文件系统

**分布式文件系统**的两个特点：

1. **文件非常大**，如 TB 级的文件。

2. **文件极少更新**，额外数据追加到文件尾部。

海量(enormous)冷(rarely updated)数据如何存储？使用**分布式文件系统**存储

1. 硬盘够不够大：分布式存储
2. 硬盘够不够安全：冗余机制

分布式文件系统原理

1. 使用普通廉价的硬件，**硬件故障是常态**而不是异常
2. 流式数据访问：数据批量读取而非随机读写，擅长做**数据分析**而不是数据处理
3. 文件**非常大**，大规模数据集
4. 文件块有**副本**
5. 简单一致性模型，**一次性写多次读**，一旦写入不能修改。**现在只允许追加操作**
6. 文件是**文件块 chunk**128M，而不是文件整体存放

分布式文件系统的组成

1. Name Node and Secondary Name Node: edit logs and fsimage
2. Data Node

主要的故障模式：

1. 单节点故障(比如某节点上的硬盘发生崩溃)

2. 单机架故障(比如机架内节点间的互连网络及当前机架到其他机架的互连网络发生故障)。

<!-- ## 第四周-MapReduce 框架 -->

### 2.2 MapReduce 框架

智能用于并行计算，不可以用于串行计算。

1. 不同的 Map 任务和 Reduce 任务之间不会进行通信
2. 用户不能显式地进行机器之间的信息交换，所有通信依赖 MapReduce 框架
3. MapReduce 不适用于**实时**的计算，如计算当前某产品的抢购数量
4. MapReduce 采用“分而治之”策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片（split），这些分片可以被多个 Map 任务并行处理
   MapReduce 设计的一个理念就是“**计算向数据靠拢**”，而不是“数据向计算靠拢”，因为，移动数据需要大量的网络传输开销
5. Reduce 任务需要远远小于 Map 的数目，同时应当多于 Reducer 的数目
6. 用于搜索领域，解决海量数据的计算问题
7. MapReduce 将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数：Map 和 Reduce
8. 编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算
9. MapReduce 框架采用了 Master/Worker 架构，包括一个 Master 和若干个 Worker。
10. Hadoop 框架是用 Java 实现的，但是，MapReduce 应用程序则不一定要用 Java 来写

<!-- ## 第五周-MapReduce 应用 -->

### 2.2 MapReduce 应用

1. 字数统计
2. 关系代数计算
   1. 选择，投影，
   2. 并、交、差，
   3. 自然连接（一定要存在一个公共属性），分组和聚合
3. 基于 MapReduce 的矩阵 M-向量 v 乘法：
   1. v 能入内存：
   2. v 不能放入内存：将 v 分成水平条将 M 分成垂直条分别放入内存计算再合并
4. 基于 MapReduce 的矩阵-矩阵乘法：可以通过两个 MapReduce 的串联，第一个是一个**自然连接**，第二个是一个**分组聚合**

![](pics/没有定义组合器的Reduce.png)
![](pics/定义组合器的Reduce.png)

## 3 相似项发现

相似项是一个基本数据挖掘问题，应用于去重，查找文档相似度。

![](./pics/相似项发现思路.png)

- 相似项发现问题表述为寻找具有相对较大交集的集合问题;
- 文档的 “shingling”技术;
- 最小哈希(minhashing)技术，它能够对大集合进行压缩，并且可以基于压缩后的结果推导原始集合的相似度;
- 局部敏感哈希(Locality-Sensitive Hashing，简称 LSH)的技术.

先找出可能的候选对相似文档集合，然后基于该集合发现真正的相似文档。

1. 每篇文档构建其 k-shingle 集合，并映射成更短的桶编号。
2. 选择最小哈希签名的长度 n，计算所有文档的最小哈希签名。
3. 选择阈值 t 来定义应该达到的相似程度使之被看做是预期的“相似对”。选择行条数 b 和每个行条中的行数 r，使得 br=n，而阈值 t 近似等于(1/b)1/r；
4. 利用 LSH 技术来构建候选对；
5. 如果签名足够相似，则检查文档本身看它们是否真正相似。

### 3.1 近邻搜索的应用

#### 集合的相似度

**Jaccard 相似度衡量集合的相似度**(要知道如何计算)

Jaccard 相似度主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具 体值的大小，只能获得“是否相同”这个结果，所以 Jaccard 相似度只关心个体间共同具有的特征是否一致这个问题。

$$
SIM (C1, C2) = \frac{|C1 \cap C2|}{|C1 \cup C2|}.
$$

如果是包运算，并集不要合并相同的元素，直接元素数相加

#### 文档的相似度的应用

- 抄袭文档
- 镜像页面
- 同源新闻稿

#### 协同过滤-------一个集合相似问题

另一类非常重要的集合相似度应用称为协同过滤( colla-borative filtering )，在协同过滤中，系统会向用户推荐相似兴趣用户所喜欢的那些项。

在线购物，电影评级中的用户的兴趣相似

### 3.2 文档的 Shingling

- 3.2.1 k-single：注意是**集合**，不能有重复
- 3.2.2 shingle 大小的选择：**k 应足够大，以保证任意给定的 shingle 出现在任意文档中的概率较低**。邮件 5，论文 9。
- 3.2.3 对 shingle 进行哈希：9-single 通常映射到 4 个字节
- 3.2.4 基于词的 shingle：Shingle 定义为一个**停用词**加上后续的两个词（不管是否是停用词）

### 3.3 保持相似度的集合摘要表示

- Shingle 集合非常大。一个 4 shingle 集合也是原始文档的 4 倍。如果有数百万文档，很可能不能将这些文档的 shingle 集合都放入内存中。
- 目标：“**签名**”( signature) 可以看成一种特征，需要保持变换后的相似度接近于原始的 Jaccard 相似度。
- 结果：基于最终签名集合得到的原始文档 Jaccard 相似度的估计值与真实值的差异也就在几个百分点之内。

#### 3.3.1 集合的矩阵表示

**特征矩阵**并非数据真正的存储方式，但是作为数据可视化的一种方式则是非常有用的。大部分是 0，只需存储 1 所在的位置。

#### 3.3.2 最小哈希

- 可以看成是一个特征提取的过程。
- 集合的签名由大量计算(比如数百次)的结果组成，而每次计算是特征矩阵的最小哈希(minhashing)过程
- 选择行的一个排列转换。任意一列的最小哈希值是在排列转换后的行排列次序下第一个列值为 1 的行的行号。
- 每个排列转换定义了一个最小哈希函数 h。

任意一列的最小哈希值是在排列转换后的行排列次序下第一个列值为 1 的行的行号。

#### 3.3.3 最小哈希及 Jaccard 相似度

最小哈希及 Jaccard 相似度：两个集合经随机排列**转换之后**得到的两个最小哈希值相等的概率**等于**这两个集合的 Jaccard 相似度。

多计算几次最小哈希

#### 最小哈希签名

特征矩阵表示 M。随机选择 n 个排列转换用于矩阵 M 的行处理。其中 n 一般为一百或几百。对于集合 S 对应的列，分别调用这些排列转换所决定的最小哈希函数 h1，h2，h3，.......，hn，则可以构建 S 的最小哈希签名向量。

#### 最小哈希签名的计算 （重点内容）

通过一个随机哈希函数来模拟随机排列转化的效果，该函数将行号映射到与行数目大致相等的数量的桶中；

全集{a, b, c, d, e}，
S1={a, d}, s2={c}, s3={b, d, e}, s4={a, c, d}

![](./pics/集合的矩阵表示.png)

![](./pics/最小哈希签名矩阵计算.png)

签名矩阵中 S1 和 S4 对应的列向量完全相同，因此我们可以猜测 SIM (S1, S4)=1.0。如果回到图 3-4，我们会发现 S1 和 S4 的真实 Jaccard 相似度为 2/3。
因为本例规模太小，所以并不足以说明在大规模数据情况下估计值和真实值相近的规律

### 3.4 文档的局部敏感哈希算法 LSH

最小哈希将大文档压缩成小的签名并同时保持任意对文档之间的预期相似度，但是高效寻找具有最大相似度的文档对仍然是不可能的。主要原因在于，即使文档本身的数目并不很大，但需要比较的文档对的数目可能太大。

解决办法：**局部敏感哈希**(Locality-Sensitive Hashing, LSH)或**近邻搜索**(near-neighbor search)的一般性理论。

**基本思想**：对目标项进行多次哈希处理，**使得相似项会比不相似项更可能哈希到同一个桶中。**
将至少有一次哈希到同一桶中的文档对看作是候选对，只去检查这些候选对之间的相似度。
我们希望大部分不相似的文档对不会哈希到相同的桶中，这样就永远不需要检查它们的相似度。

**LSH 步骤**：

1. 把签名矩阵分成多个行条，对每个行条使用哈希函数。
2. 具有相同部分的列将被哈希到同一个桶中。
3. 只考察那些哈希到同一个桶里面的列的相似性。

<!-- ## 第六周-近邻搜索的应用 -->

### 3.5 距离测度

1. 欧氏距离
2. Jaccard 距离 D(x,y)= 1-SIM(x,y)
3. 余弦距离：
4. 在有限维的欧式空间，每个点表示一个向量。
   两个向量的夹角定义为：两个向量的内积与两个点之间的欧式距离的比值。再用这个比值去反余弦，获得夹角度数
5. 编辑距离：适用于字符串比较，两个字符串 x, y 的编辑距离等于将 x 转换为 y 所需要的单字符插入及删除操作的最小数目。
6. 汉明距离：汉明距离定义为两个向量中不同分量的个数。

X 和 y 的最长公共子序列（LCS）：通过在 x 和 y 的某些位置进行删除，得到 x,y 的最长公共字符串。
编辑距离等于 x 的长度,y 的长度的和减去两倍的 LCS
如 x=abcde, y=acfdeg。LCS=acde
编辑距离=5+6-2×4=3

### 3.8LSH 函数的应用

实体关联(Entity Resolution) 这项任务需要解决的主要问题在于，记录之间的相似度计算既不是纯粹的集合相似度也不是纯粹的向量相似度

- 指纹匹配
- 相似新闻报道检测

## 4 流数据挖掘

### 流数据的概念

流数据处理的是**部分数据**而不是全部数据。传感器，图像，互联网

- Hadoop 擅长批处理，不适合流计算。
- **流计算**：实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息
- 流计算秉承一个基本理念，即**数据的价值随着时间的流逝而降低**

流数据的特点：无穷无尽、非平稳

流查询：

- 固定查询：从不停止
- Ad-hoc 查询：存储数据流的合适部分，为查询作准备；根据存储的内容应答，只执行一次

流数据处理的限制

- 流元素的**分发速度**通常很快。所以，必须要对元素进行实时处理，否则就会永远失去处理它们的机会，除非访问归档存储器。
- 流处理算法通常在**内存**中执行，一般不会或者极少访问二级存储器，这一点相当重要。
- 即使当数据流很慢(参考 4.1.2 节的传感器数据的例子)时，也可能存在多个这样的数据流。即使每个流本身能够基于很小的内存就能处理，但所有数据流的内存需求加在一起可能就很容易超过内存的可用容量。

流数据处理策略，两个一般化的结论：

- 通常情况下，获得问题的近似解比精确解要高效得多。
- 一系列与哈希相关的技术被证明十分有用。

### 4.2 流当中的数据抽样

<!-- **简单随机抽样**：从总体 N 个单位中随机地抽取 n 个单位作为样本，使得每一个样本都有相同的概率被抽中。通常只是在总体单位之间差异程度较小和数目较少时，才采用这种方法。

**系统抽样**：将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。先从数字 1 到 k 之间随机抽取一个数字 r 作为初始单位，以后依次取 r+k、r+2k……等单位。

**分层抽样**：将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本，保证样本的结构与总体的结构比较相近，从而提高估计的精度。

**整群抽样**：将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。抽样时只需群的抽样框，可简化工作量，缺点是估计的精度较差。 -->

#### 4.2.2 代表性样本的获取

不能从每个用户的搜索查询的抽样样本中得到正确答案。
必须要挑出 1/10 的**用户**并将他们的所有搜索查询放入样本中，而不考虑其他用户的搜索查询。

### 4.3 流过滤

#### 4.3.2 布隆过滤器

布隆过滤器（Bloom Filter）是一种概率空间高效的数据结构。用于检索一个元素是否在一个集合中。
存在“在集合内（可能错误）”和“不在集合内（绝对不在集合内）”两种情况。**应用**：垃圾邮件、判断犯罪嫌疑人，拼写检查

**核心思想**就是利用多个不同的 Hash 函数来解决“冲突”。

集合 S 有 m 个元素，位数组容量为 n，而哈希函数有 k 个
即 靶位的数目 x=n， 飞镖的数目为 y=km
让所有的 k 个布隆过滤器都为 1 的概率是

$$
(1−e^{−\frac{k m}{n})^k
$$

### 4.4 流中独立元素的数目统计

问题描述：假定流元素选自某个全集。我们想知道流当中从头或某个已知的过去时刻开始所出现的**不同的元素数目**。
S={x1，x2，x3,……,xN}, 找出 N

目标：在合理的内存空间下，得到我们想要的近似结果。

#### 4.4.2 FM（Flajolet-Martin）算法

基本思想：如果流中看到的不同元素越多，那么我们看到的不同哈希值也会越多。在看到的不同哈希值越多的同时，也越可能看到其中有一个值变得“异常”。

假定哈希函数 H(e)能够把元素 e 映射到[0, 2m-1]区间上；再假定函数 TailZero(x)能够计算正整数 x 的二进制表示中末尾连续的 0 的个数，

我们对每个元素 e 计算 TailZero(H(e))，并求出最大的 TailZero(H(e))记为 Max，那么对于独立元素数目的估计为$2^{Max}$。

m 的估计值 2R 不可能过高或过低。

#### 4.4.3 组合估计

我们可以将两种策略组合起来：
首先将哈希函数分成小组，每个组内取平均值(单独使用的缺点：容易过高)。
然后在所有平均值中取中位数(单独使用的缺点：永远都是 2 的幂，无法得到精确估计值)。

#### 4.4.4 空间需求

唯一需要在内存保存的是每个哈希函数所对应的一个整数。

### 4.5 矩估计

二阶矩估计的计算

$$
j=\sum_{\text{for every element}}{n^2}
$$

## 5 链接分析

- PageRank ：是一种对每个网页分配一个实数值 PageRank 的算法。一个网页的 PageRank 值度量了该网页的重要性，或者作为好的搜索查询应答结果的可能性。
- 随机冲浪模型: PageRank 的计算可以想象成对许多随机冲浪者的行为模拟。每个冲浪者从某个随机页面开始，每下一步都随机地访问当前页面所链接的页面，随机冲浪者倾向于停留在有用的页面上。
- 抽税机制 让随机冲浪者每次都有一个小的概率随机跳转。
- 极大规模矩阵-向量乘法：MapReduce 分布式计算，计算向数据靠拢

PageRank 主要解决**词项作弊**的问题，

### 5.1.2 PageRank 的定义

假定随机冲浪者处于 n 个网页的初始概率相等，那么初始概率分布向量为 $v_0$，其中每个元素值为 1/n。根据转移矩阵 M，我们可以求得 i 步之后的概率分布向量 $M^iv_0$

如果是平稳的，条件是：在不可约、非周期、正常返的条件下，马尔可夫链拥有唯一稳态分布。

- 图是强连通(strongly connected)图，即可以从任一节点到达其他节点;
- 图不存在终止点(dead-end )，即那些不存在出链的节点。

#### 5.1.4 避免终止点

可以使用抽水法，也可以删除终止点，注意计算结果不再是概率分布

#### 5.1.5 采集器陷阱

采集器陷阱及“抽税”法 v’ = βMv+(1- β)e/n 含义： β 常数 0.8-0.9

### 5.2PageRank 快速计算

使用 MapReduce 来优化 PageRank 计算组合器的引入会导致内存振荡，解决方案是矩阵分块

#### 5.3 面向主题的 pagerank

需要知道用户的偏好

### 5.4 链接作弊

#### 5.4.1 垃圾农场的架构

按照作弊者的观点：

- 不可达网页或不可达页(inaccessible page)即作弊者无法影响的网页。
- 可达网页或可达页(accessible page)这些网页虽然不受作弊者控制，但是作弊者可以影响它们。
- 自有网页或自有页(own page )作弊者拥有并完全控制的网页。

#### 5.4.3 与链接作弊的斗争

（1）查找垃圾农场结构，即其中某个网页链向大量网页，而这些网页又都回指这种结构。搜索引擎肯定能够找出满足这些结构的网页并将它们从索引中消除。从本质上而言，结构有无数变形，所以作弊者和搜索引擎之间的斗争可能将会存在很长一段时间。

（2）在不依赖链接作弊定位的同时去掉这些链接，修改 PageRank 算法的定义来自动降低链接作弊网页的重要度。

- TrustRank：面向主题的 PageRank 的一种变形，设计为可以降低垃圾网页的得分;
- 垃圾质量( spam mass)：能够识别可能为垃圾的网页并允许搜索引擎去掉这些网页或者大力降低这些网页的 PageRank 的计算。

### 5.5 导航页与权威页

- 由于某些网页提供了有关某个主题的信息，因此它们具有非常重要的价值，这些网页被称为**权威页**(authority)。
- 有些网页并不提供有关任何主题的信息，但是由于它们可以给出找到有关该主题的网页的信息，所以它们也具有重要价值。这些网页称为**导航页**(hub）。

## 6 频繁项集

### 6.1.1 频繁项集的定义

- 数据的购物篮模型(market-basket model )用于描述两类对象之间一种常见形式的多对多关系。
- 数据一类对象是项(item )，另一类对象是购物篮(basket )，后者有时称为“交易”( transaction )。
- 每个购物篮由多个项组成的集合(称为项集，itemset )构成，通常我们都假设一个购物篮中项的总数目较小，相对于所有项的总数目而言要小得多。
- 购物篮的数目通常假设很大，导致在内存中无法存放 。
- 整个数据假定由一个购物篮序列构成的文件来表示。(超市购物小票记录)

直观上看，**一个在多个购物篮中出现的项集称为“频繁”项集**。  
公式上，如果 I 是一个项集，I 的支持度(support)是指包含 I(即 I 是购物篮中项集的子集)的购物篮数目。
假定有个支持度阈值(support threshold )S。如果 I 的支持度不小于 S，则称 I 是频繁项集(frequent itemset )。

#### 6.1.2 频繁项集的应用

- 关联概念(Related concepts)
- 文档抄袭(Plagiarism)
- 生物标志物(Biomarker)

#### 6.1.3 关联规则

抽取结果往往采用 if-then 形式的规则集合来表示，这些规则称为关联规则(association rule )。
一条关联规则的形式为 I→j ,其中 I 是一个项集，而 j 是一个项。该关联规则的意义是，如果 I 中所有项出现在某个购物篮的话，那么 j“有可能”也出现在这一购物篮。

- 支持度：刻画项集出现频度，必须频繁才有意义
- 可信度：刻画从 I 到 j 的规则强度
- 兴趣度：刻画是否有意义

定义规则的**可信度**(confidence)来给出“有可能”这个概念的形式化定义。
规则 I →j 的可信度: 集合 I∪{j}的支持度与 I 的支持度的比值。也就是，所有包含 I 的购物篮中同时包含 j 的购物篮的比例。

定义 A → j 的**兴趣度** = A → j 的可信度 - j 的支持度/|购物篮|
当这个值很高或者是绝对值很大的负值都是具有意义的。
前者意味着购物篮中 A 的存在在某种程度上促进了 j 的存在
后者意味着 A 的存在会抑制 j 的存在。

出现“dog”的 7 个购物篮中有 5 个包含“cat"，因此规则{dog}→cat 的可信度为 5/7。"cat'’出现在所有 8 个购物篮中的 6 个，因此规则的兴趣度为 5/7-3/4=-0.036，即基本为 0。

### 6.2 购物篮及 A-Priori 算法

#### 6.2.3 项集的单调性

A-priori 算法的高效性主要归功于某个观察结果，即项集的单调性(monotanicity ):

- 如果项集 I 是频繁的，那么其所有的子集都是频繁的。
- 有一个子集不是频繁项集，那它也不可能是频繁项集.

## 7 聚类

- 聚类：从数据中分成若干组：每一组在群里很相似,不同组的相似很小
- 无监督学习:无标签,数据驱动

## 7.1 聚类技术介绍

### 7.1.2 聚类策略

按照策略划分:

- **层次算法**。这类算法一开始将每个点都看成一个簇。簇与簇之间按照接近度(closeness)来组合，而接近度可以基于“接近”的不同含义采用不同的定义。当进一步的组合导致多个原因之一下的非期望结果时，上述组合过程结束。
- **点分配算法**，即按照某个顺序依次考虑每个点，并将它分配到最适合的簇中。该过程通常都有一个短暂的初始簇估计阶段。一些变形算法允许临时的簇合并或分裂过程，或者当点为离群点(离当前任何簇的距离都很远的点)时允许不将该点分配到任何簇中。

#### 7.1.3 维数灾难

维数灾难：高维欧氏空间和非欧空间下的点往往表现得和直觉不太一致，称之为维数灾难。
在高维空间下，几乎所有的点对之间的距离都差不多相等。
几乎任意的两个向量之间都是近似正交的。

- 在高维空间下，几乎所有的点对之间的距离都差不多相等。
- 几乎任意的两个向量之间都是近似正交的。

### 7.3 k-均值算法

一种点分配算法

1. 从数据中随机抽取 k 个点作为初始聚类的中心，由这个中心代表各个聚类
2. 计算数据中所有的点到这 k 个点的距离，将点归到离其最近的聚类里
3. 调整聚类中心，即将聚类的中心移动到聚类的几何中心（即平均值）处，也就是 k-means 中的 mean 的含义
4. 重复第 2、3 步直到聚类的中心不再移动，此时算法收敛

## 8 Web 广告

#### 8.1.1 Web 的广告机会

Web 广告的主要场景：

- 一些网站，如 eBay, 新浪门户等，允许广告商以免费、付费或委托方式**直接投放广告**。
- 很多 Web 网站上的**展示广告**(display ad)。广告商按照每展示一次(某个用户下载一次网页则认为该网页上的广告被展示一次)的固定费率付费。通常，即使是同一个用户对网页的第二次下载，也*会导致一个不同的广告展示*。
- 诸如 Amazon 的在线商店在很多**上下文中都显示广告**。这些广告并非由广告商品的生产者来付费，而是由在线商店选出，以最大化顾客对商品感兴趣的概率。
- **搜索广告**(search ad)包含在搜索结果中。广告商要为某些查询进行投标以获得在搜索结果中展示广告的权利，但是他们只在广告被点击的情况下才付费。显示广告的选择过程非常复杂。
- 新闻/媒体网站上的广告

### 8.2 在线算法

离线算法( off line algorithms)，是指基于在执行算法前输入数据已知的基本假设，也就是说，对于一个离线算法，在开始时就需要知道问题的所有输入数据，而且在解决一个问题后就要立即输出结果。

在线算法是指它可以以序列化的方式一个个的处理输入，也就是说在开始时并不需要已经知道所有的输入。
执行算法时，不知道所有的输入，没有离线效果好

贪心算法（**一种在线算法**），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法。比如在旅行推销员问题中，如果旅行员每次都选择最近的城市，那这就是一种贪心算法。

#### 8.2.3 竞争率

在线算法不如最佳的离线算法效果那么好。
定义**竞争率**（competitive ratio）：存在某个小于 1 的常数 c，使得对于任一输入，一个具体的在线算法的结果至少是最优离线算法结果的 c 倍。

## 9 推荐系统

### 推荐系统和搜索引擎的异同点

- 相同点：都是一种帮助用户快速**发现有用信息**的工具
- 不同点：
  - 搜索引擎需要用户主动提供准确的关键词来寻找信息
  - 推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模

从某种意义上说，推荐系统和搜索引擎对于用户来说是两个互补的工具
搜索引擎满足了用户有明确目的时的主动查找需求
推荐系统能够在用户没有明确目的的时候帮助他们发现感兴趣的新内容

### 推荐系统可以分成两大类

基于**内容**的系统(Content-based System)：这类系统主要考察的是推荐项的性质。项之间的相似度通过计算它们的属性之间的相似度来确定。例如，如果一个 Netflix 的用户观看了多部西部牛仔片，那么系统就会将数据库中属于“西部牛仔”类的电影推荐给该用户。
**协同过滤**系统(Collaborative Filtering System)：这类系统通过计算用户或/和项之间的相似度来推荐项。与某用户相似的用户所喜欢的项会推荐给该用户。
